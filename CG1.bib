
@inproceedings{zhang_oag-bench_2024,
	address = {Barcelona Spain},
	title = {{OAG}-{Bench}: {A} {Human}-{Curated} {Benchmark} for {Academic} {Graph} {Mining}},
	isbn = {9798400704901},
	shorttitle = {{OAG}-{Bench}},
	url = {https://dl.acm.org/doi/10.1145/3637528.3672354},
	doi = {10.1145/3637528.3672354},
	language = {en},
	urldate = {2024-10-26},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Zhang, Fanjin and Shi, Shijie and Zhu, Yifan and Chen, Bo and Cen, Yukuo and Yu, Jifan and Chen, Yelin and Wang, Lulu and Zhao, Qingfei and Cheng, Yuqing and Han, Tianyi and An, Yuwei and Zhang, Dan and Tam, Weng Lam and Cao, Kun and Pang, Yunhe and Guan, Xinyu and Yuan, Huihui and Song, Jian and Li, Xiaoyan and Dong, Yuxiao and Tang, Jie},
	month = aug,
	year = {2024},
	pages = {6214--6225},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\W6PLQC2S\\Zhang 等 - 2024 - OAG-Bench A Human-Curated Benchmark for Academic .pdf:application/pdf},
}

@article{huo_hierarchical_2023,
	title = {Hierarchical spatio–temporal graph convolutional networks and transformer network for traffic flow forecasting},
	volume = {24},
	url = {https://ieeexplore.ieee.org/abstract/document/10012451/},
	number = {4},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Huo, Guangyu and Zhang, Yong and Wang, Boyue and Gao, Junbin and Hu, Yongli and Yin, Baocai},
	year = {2023},
	note = {Publisher: IEEE},
	keywords = {Task analysis, Convolution, Forecasting, Graph convolutional networks, Network topology, Predictive models, Roads, traffic data forecasting, transformer, Transformers},
	pages = {3855--3867},
	file = {Full Text PDF:C\:\\Users\\Y9000X\\Zotero\\storage\\BW43GS2I\\Huo 等 - 2023 - Hierarchical Spatio–Temporal Graph Convolutional N.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Y9000X\\Zotero\\storage\\LJLYSLEP\\10012451.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\Y9000X\\Zotero\\storage\\ZS3HUZ6E\\10012451.html:text/html},
}

@article{garbin_voltemorph_2024,
	title = {{VolTeMorph}: {Real}‐time, {Controllable} and {Generalizable} {Animation} of {Volumetric} {Representations}},
	volume = {43},
	issn = {0167-7055, 1467-8659},
	shorttitle = {{VolTeMorph}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.15117},
	doi = {10.1111/cgf.15117},
	abstract = {Abstract
            The recent increase in popularity of volumetric representations for scene reconstruction and novel view synthesis has put renewed focus on animating volumetric content at high visual quality and in real‐time. While implicit deformation methods based on learned functions can produce impressive results, they are ‘black boxes’ to artists and content creators, they require large amounts of training data to generalize meaningfully, and they do not produce realistic extrapolations outside of this data. In this work, we solve these issues by introducing a volume deformation method which is real‐time even for complex deformations, easy to edit with off‐the‐shelf software and can extrapolate convincingly. To demonstrate the versatility of our method, we apply it in two scenarios: physics‐based object deformation and telepresence where avatars are controlled using blendshapes. We also perform thorough experiments showing that our method compares favourably to both volumetric approaches combined with implicit deformation and methods based on mesh deformation.},
	language = {en},
	number = {6},
	urldate = {2024-10-26},
	journal = {Computer Graphics Forum},
	author = {Garbin, Stephan J. and Kowalski, Marek and Estellers, Virginia and Szymanowicz, Stanislaw and Rezaeifar, Shideh and Shen, Jingjing and Johnson, Matthew A. and Valentin, Julien},
	month = sep,
	year = {2024},
	pages = {e15117},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\XFCGYC6F\\Garbin 等 - 2024 - VolTeMorph Real‐time, Controllable and Generaliza.pdf:application/pdf},
}

@article{wang_proteusnerf_2024,
	title = {{ProteusNeRF}: {Fast} {Lightweight} {NeRF} {Editing} using {3D}-{Aware} {Image} {Context}},
	volume = {7},
	issn = {2577-6193},
	shorttitle = {{ProteusNeRF}},
	url = {https://dl.acm.org/doi/10.1145/3651290},
	doi = {10.1145/3651290},
	abstract = {Neural Radiance Fields (NeRFs) have recently emerged as a popular option for photo-realistic object capture due to their ability to faithfully capture high-fidelity volumetric content even from handheld video input. Although much research has been devoted to efficient optimization leading to real-time training and rendering, options for interactive editing NeRFs remain limited. We present a very simple but effective neural network architecture that is fast and efficient while maintaining a low memory footprint. This architecture can be incrementally guided through user-friendly image-based edits. Our representation allows straightforward object selection via semantic feature distillation at the training stage. More importantly, we propose a local 3D-aware image context to facilitate view-consistent image editing that can then be distilled into fine-tuned NeRFs, via geometric and appearance adjustments. We evaluate our setup on a variety of examples to demonstrate appearance and geometric edits and report 10-30x speedup over concurrent work focusing on text-guided NeRF editing. Video results and code can be found on our project webpage at https://proteusnerf.github.io.},
	language = {en},
	number = {1},
	urldate = {2024-10-26},
	journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
	author = {Wang, Binglun and Dutt, Niladri Shekhar and Mitra, Niloy J.},
	month = may,
	year = {2024},
	pages = {1--17},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\6MLF4AQ2\\Wang 等 - 2024 - ProteusNeRF Fast Lightweight NeRF Editing using 3.pdf:application/pdf},
}

@article{deng_solving_2023,
	title = {Solving {Graph} {Problems} {Using} {Gaussian} {Boson} {Sampling}},
	volume = {130},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.130.190601},
	doi = {10.1103/PhysRevLett.130.190601},
	language = {en},
	number = {19},
	urldate = {2024-10-26},
	journal = {Physical Review Letters},
	author = {Deng, Yu-Hao and Gong, Si-Qiu and Gu, Yi-Chao and Zhang, Zhi-Jiong and Liu, Hua-Liang and Su, Hao and Tang, Hao-Yang and Xu, Jia-Min and Jia, Meng-Hao and Chen, Ming-Cheng and Zhong, Han-Sen and Wang, Hui and Yan, Jiarong and Hu, Yi and Huang, Jia and Zhang, Wei-Jun and Li, Hao and Jiang, Xiao and You, Lixing and Wang, Zhen and Li, Li and Liu, Nai-Le and Lu, Chao-Yang and Pan, Jian-Wei},
	month = may,
	year = {2023},
	pages = {190601},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\4BFJUGUB\\Deng 等 - 2023 - Solving Graph Problems Using Gaussian Boson Sampli.pdf:application/pdf},
}

@inproceedings{rakotosaona_nerfmeshing_2024,
	title = {Nerfmeshing: {Distilling} neural radiance fields into geometrically-accurate 3d meshes},
	shorttitle = {Nerfmeshing},
	url = {https://ieeexplore.ieee.org/abstract/document/10550893/},
	urldate = {2024-10-26},
	booktitle = {2024 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Rakotosaona, Marie-Julie and Manhardt, Fabian and Arroyo, Diego Martin and Niemeyer, Michael and Kundu, Abhijit and Tombari, Federico},
	year = {2024},
	pages = {1156--1165},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\98M2J5HM\\Rakotosaona 等 - 2024 - Nerfmeshing Distilling neural radiance fields int.pdf:application/pdf},
}

@article{jamali_automated_2024,
	title = {Automated model building and protein identification in cryo-{EM} maps},
	volume = {628},
	url = {https://www.nature.com/articles/s41586-024-07215-4},
	number = {8007},
	urldate = {2024-10-26},
	journal = {Nature},
	author = {Jamali, Kiarash and Käll, Lukas and Zhang, Rui and Brown, Alan and Kimanius, Dari and Scheres, Sjors HW},
	year = {2024},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {450--457},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\KKLLZXPJ\\Jamali 等 - 2024 - Automated model building and protein identificatio.pdf:application/pdf;Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\LN5ES435\\Jamali 等 - 2024 - Automated model building and protein identificatio.pdf:application/pdf},
}

@incollection{ogudo_graph-based_2023,
	address = {Singapore},
	title = {A {Graph}-{Based} {Model} for {Discovering} {Host}-{Based} {Hook} {Attacks}},
	volume = {558},
	isbn = {978-981-19687-9-2 978-981-19688-0-8},
	url = {https://link.springer.com/10.1007/978-981-19-6880-8_1},
	language = {en},
	urldate = {2024-10-26},
	booktitle = {Smart {Technologies} in {Data} {Science} and {Communication}},
	publisher = {Springer Nature Singapore},
	author = {Pandiaraja, P. and Muthumanickam, K. and Palani Kumar, R.},
	editor = {Ogudo, Kingsley A. and Saha, Sanjoy Kumar and Bhattacharyya, Debnath},
	year = {2023},
	doi = {10.1007/978-981-19-6880-8_1},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {1--13},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\MUD7MR6Z\\Pandiaraja 等 - 2023 - A Graph-Based Model for Discovering Host-Based Hoo.pdf:application/pdf},
}

@article{dong_laplacian2mesh_2023,
	title = {Laplacian2mesh: {Laplacian}-based mesh understanding},
	shorttitle = {Laplacian2mesh},
	url = {https://ieeexplore.ieee.org/abstract/document/10076837/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Dong, Qiujie and Wang, Zixiong and Li, Manyi and Gao, Junjie and Chen, Shuangmin and Shu, Zhenyu and Xin, Shiqing and Tu, Changhe and Wang, Wenping},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\FNNEIKHA\\Dong 等 - 2023 - Laplacian2mesh Laplacian-based mesh understanding.pdf:application/pdf},
}

@article{song_nerfplayer_2023,
	title = {Nerfplayer: {A} streamable dynamic scene representation with decomposed neural radiance fields},
	volume = {29},
	shorttitle = {Nerfplayer},
	url = {https://ieeexplore.ieee.org/abstract/document/10049689/},
	number = {5},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Song, Liangchen and Chen, Anpei and Li, Zhong and Chen, Zhang and Chen, Lele and Yuan, Junsong and Xu, Yi and Geiger, Andreas},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {2732--2742},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\4P3YDLLP\\Song 等 - 2023 - Nerfplayer A streamable dynamic scene representat.pdf:application/pdf},
}

@article{yeh_attentionviz_2023,
	title = {Attentionviz: {A} global view of transformer attention},
	shorttitle = {Attentionviz},
	url = {https://ieeexplore.ieee.org/abstract/document/10297591/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Yeh, Catherine and Chen, Yida and Wu, Aoyu and Chen, Cynthia and Viégas, Fernanda and Wattenberg, Martin},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\77VVBYD2\\Yeh 等 - 2023 - Attentionviz A global view of transformer attenti.pdf:application/pdf},
}

@article{kourtesis_cybersickness_2023,
	title = {Cybersickness, cognition, \& motor skills: {The} effects of music, gender, and gaming experience},
	volume = {29},
	shorttitle = {Cybersickness, cognition, \& motor skills},
	url = {https://ieeexplore.ieee.org/abstract/document/10049731/},
	number = {5},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kourtesis, Panagiotis and Amir, Rayaan and Linnell, Josie and Argelaguet, Ferran and MacPherson, Sarah E.},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {2326--2336},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\XTUNIFB9\\Kourtesis 等 - 2023 - Cybersickness, cognition, & motor skills The effe.pdf:application/pdf},
}

@article{feng_xnli_2023,
	title = {Xnli: {Explaining} and diagnosing nli-based visual data analysis},
	shorttitle = {Xnli},
	url = {https://ieeexplore.ieee.org/abstract/document/10026499/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Yingchaojie and Wang, Xingbo and Pan, Bo and Wong, Kam Kwai and Ren, Yi and Liu, Shi and Yan, Zihan and Ma, Yuxin and Qu, Huamin and Chen, Wei},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\GVKZQYJH\\Feng 等 - 2023 - Xnli Explaining and diagnosing nli-based visual d.pdf:application/pdf},
}

@article{li_multimodal_2024,
	title = {Multimodal foundation models: {From} specialists to general-purpose assistants},
	volume = {16},
	shorttitle = {Multimodal foundation models},
	url = {https://www.nowpublishers.com/article/Details/CGV-110},
	number = {1-2},
	urldate = {2024-10-26},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
	year = {2024},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--214},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\X9V2CDLU\\Li 等 - 2024 - Multimodal foundation models From specialists to .pdf:application/pdf},
}

@article{habermann_hdhumans_2023,
	title = {{HDHumans}: {A} {Hybrid} {Approach} for {High}-fidelity {Digital} {Humans}},
	volume = {6},
	issn = {2577-6193},
	shorttitle = {{HDHumans}},
	url = {https://dl.acm.org/doi/10.1145/3606927},
	doi = {10.1145/3606927},
	abstract = {Photo-real digital human avatars are of enormous importance in graphics, as they enable immersive communication over the globe, improve gaming and entertainment experiences, and can be particularly beneficial for AR and VR settings. However, current avatar generation approaches either fall short in high-fidelity novel view synthesis, generalization to novel motions, reproduction of loose clothing, or they cannot render characters at the high resolution offered by modern displays. To this end, we propose HDHumans, which is the first method for HD human character synthesis that jointly produces an accurate and temporally coherent 3D deforming surface and highly photo-realistic images of arbitrary novel views and of motions not seen at training time. At the technical core, our method tightly integrates a classical deforming character template with neural radiance fields (NeRF). Our method is carefully designed to achieve a synergy between classical surface deformation and a NeRF. First, the template guides the NeRF, which allows synthesizing novel views of a highly dynamic and articulated character and even enables the synthesis of novel motions. Second, we also leverage the dense pointclouds resulting from the NeRF to further improve the deforming surface via 3D-to-3D supervision. We outperform the state of the art quantitatively and qualitatively in terms of synthesis quality and resolution, as well as the quality of 3D surface reconstruction.},
	language = {en},
	number = {3},
	urldate = {2024-10-26},
	journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
	author = {Habermann, Marc and Liu, Lingjie and Xu, Weipeng and Pons-Moll, Gerard and Zollhoefer, Michael and Theobalt, Christian},
	month = aug,
	year = {2023},
	pages = {1--23},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\PPSJSGAR\\Habermann 等 - 2023 - HDHumans A Hybrid Approach for High-fidelity Digi.pdf:application/pdf},
}

@article{you_decoupling_2024,
	title = {Decoupling dynamic monocular videos for dynamic view synthesis},
	url = {https://ieeexplore.ieee.org/abstract/document/10612248/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {You, Meng and Hou, Junhui},
	year = {2024},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\GXRH3VFR\\You 和 Hou - 2024 - Decoupling dynamic monocular videos for dynamic vi.pdf:application/pdf},
}

@article{weidner_systematic_2023,
	title = {A systematic review on the visualization of avatars and agents in ar \& vr displayed using head-mounted displays},
	volume = {29},
	url = {https://ieeexplore.ieee.org/abstract/document/10049669/},
	number = {5},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Weidner, Florian and Boettcher, Gerd and Arboleda, Stephanie Arevalo and Diao, Chenyao and Sinani, Luljeta and Kunert, Christian and Gerhardt, Christoph and Broll, Wolfgang and Raake, Alexander},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {2596--2606},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\I6W5II64\\Weidner 等 - 2023 - A systematic review on the visualization of avatar.pdf:application/pdf},
}

@article{wang_nerf-art_2023,
	title = {Nerf-art: {Text}-driven neural radiance fields stylization},
	shorttitle = {Nerf-art},
	url = {https://ieeexplore.ieee.org/abstract/document/10144678/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, Can and Jiang, Ruixiang and Chai, Menglei and He, Mingming and Chen, Dongdong and Liao, Jing},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\GLQKBLS2\\Wang 等 - 2023 - Nerf-art Text-driven neural radiance fields styli.pdf:application/pdf},
}

@article{jambon_nerfshop_2023,
	title = {Nerfshop: {Interactive} editing of neural radiance fields},
	volume = {6},
	shorttitle = {Nerfshop},
	url = {https://inria.hal.science/hal-04027538/},
	number = {1},
	urldate = {2024-10-26},
	journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
	author = {Jambon, Clément and Kerbl, Bernhard and Kopanas, Georgios and Diolatzis, Stavros and Leimkühler, Thomas and Drettakis, George},
	year = {2023},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\UYPQBSHQ\\Jambon 等 - 2023 - Nerfshop Interactive editing of neural radiance f.pdf:application/pdf},
}

@article{mamurova_pedagogy_2023,
	title = {Pedagogy of {Technology} and its {University}},
	url = {https://www.openconference.us/index.php/innoscience/article/view/543},
	urldate = {2024-10-26},
	journal = {Innovative Science in Modern Research},
	author = {Mamurova, Feruza Islamovna and Khodzhaeva, Nodira Sharifovna and Kadirova, Elena Vladimirovna},
	year = {2023},
	pages = {22--24},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\25TD4S5N\\Mamurova 等 - 2023 - Pedagogy of Technology and its University.pdf:application/pdf},
}

@misc{kenwright_dual-quaternion_2023,
	title = {Dual-{Quaternion} {Interpolation}},
	url = {http://arxiv.org/abs/2303.13395},
	abstract = {Transformations in the field of computer graphics and geometry are one of the most important concepts for efficient manipulation and control of objects in 2-dimensional and 3-dimensional space. Transformations take many forms each with their advantages and disadvantages. A particularly powerful tool for representing transforms in a unified form are dual-quaternions. A benefit of this unified form is the interpolation properties, which address a range of limitations (compact form that allows a rotational and translational components to be coupled). In this article, we examine various dual-quaternion interpolation options that achieve different trade-offs between computational cost, aesthetic factors and coupling dependency. Surprisingly, despite dual-quaternions being a common tool in graphics libraries, there are limited details on the interpolation details. Here we attempt to explain interpolation concept, elaborating on underpinning theories, while explaining concepts and bespoke modifications for added control.},
	urldate = {2024-10-26},
	publisher = {arXiv},
	author = {Kenwright, Benjamin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13395},
	keywords = {Computer Science - Computational Geometry},
	file = {Preprint PDF:C\:\\Users\\Y9000X\\Zotero\\storage\\FMJX8SKN\\Kenwright - 2023 - Dual-Quaternion Interpolation.pdf:application/pdf;Snapshot:C\:\\Users\\Y9000X\\Zotero\\storage\\ZUBIZ6MH\\2303.html:text/html},
}

@article{wang_effect_2023,
	title = {Effect of frame rate on user experience, performance, and simulator sickness in virtual reality},
	volume = {29},
	url = {https://ieeexplore.ieee.org/abstract/document/10049694/},
	number = {5},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, Jialin and Shi, Rongkai and Zheng, Wenxuan and Xie, Weijie and Kao, Dominic and Liang, Hai-Ning},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {2478--2488},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\NJQT9PLG\\Wang 等 - 2023 - Effect of frame rate on user experience, performan.pdf:application/pdf},
}

@article{najmiddinovna_factors_2023,
	title = {Factors {Affecting} {The} {Socio}-{Pedagogical} {Activity} {Of} {Students} {In} {Multimedia} {Lectures}},
	volume = {11},
	url = {https://giirj.com/index.php/giirj/article/view/5463},
	number = {6},
	urldate = {2024-10-26},
	journal = {Galaxy International Interdisciplinary Research Journal},
	author = {Najmiddinovna, Rahimova Feruza},
	year = {2023},
	pages = {601--603},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\29NEL25N\\Najmiddinovna - 2023 - Factors Affecting The Socio-Pedagogical Activity O.pdf:application/pdf},
}

@article{ghosh_imos_2023,
	title = {{IMoS}: {Intent}‐{Driven} {Full}‐{Body} {Motion} {Synthesis} for {Human}‐{Object} {Interactions}},
	volume = {42},
	issn = {0167-7055, 1467-8659},
	shorttitle = {{IMoS}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14739},
	doi = {10.1111/cgf.14739},
	abstract = {Abstract
            Can we make virtual characters in a scene interact with their surrounding objects through simple instructions? Is it possible to synthesize such motion plausibly with a diverse set of objects and instructions? Inspired by these questions, we present the first framework to synthesize the full‐body motion of virtual human characters performing specified actions with 3D objects placed within their reach. Our system takes textual instructions specifying the objects and the associated ‘intentions’ of the virtual characters as input and outputs diverse sequences of full‐body motions. This contrasts existing works, where full‐body action synthesis methods generally do not consider object interactions, and human‐object interaction methods focus mainly on synthesizing hand or finger movements for grasping objects. We accomplish our objective by designing an intent‐driven full‐body motion generator, which uses a pair of decoupled conditional variational auto‐regressors to learn the motion of the body parts in an autoregressive manner. We also optimize the 6‐DoF pose of the objects such that they plausibly fit within the hands of the synthesized characters. We compare our proposed method with the existing methods of motion synthesis and establish a new and stronger state‐of‐the‐art for the task of intent‐driven motion synthesis.},
	language = {en},
	number = {2},
	urldate = {2024-10-26},
	journal = {Computer Graphics Forum},
	author = {Ghosh, Anindita and Dabral, Rishabh and Golyanik, Vladislav and Theobalt, Christian and Slusallek, Philipp},
	month = may,
	year = {2023},
	pages = {1--12},
	file = {已提交版本:C\:\\Users\\Y9000X\\Zotero\\storage\\L9M2CUUJ\\Ghosh 等 - 2023 - IMoS Intent‐Driven Full‐Body Motion Synthesis for.pdf:application/pdf},
}

@article{tretschk_state_2023,
	title = {State of the {Art} in {Dense} {Monocular} {Non}‐{Rigid} {3D} {Reconstruction}},
	volume = {42},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14774},
	doi = {10.1111/cgf.14774},
	abstract = {Abstract
            
              3D reconstruction of deformable (or
              non‐rigid
              ) scenes from a set of monocular 2D image observations is a long‐standing and actively researched area of computer vision and graphics. It is an ill‐posed inverse problem, since—without additional prior assumptions—it permits infinitely many solutions leading to accurate projection to the input 2D images. Non‐rigid reconstruction is a foundational building block for downstream applications like robotics, AR/VR, or visual content creation. The key advantage of using monocular cameras is their omnipresence and availability to the end users as well as their ease of use compared to more sophisticated camera set‐ups such as stereo or multi‐view systems. This survey focuses on state‐of‐the‐art methods for dense non‐rigid 3D reconstruction of various deformable objects and composite scenes from monocular videos or sets of monocular views. It reviews the fundamentals of 3D reconstruction and deformation modeling from 2D image observations. We then start from general methods—that handle arbitrary scenes and make only a few prior assumptions—and proceed towards techniques making stronger assumptions about the observed objects and types of deformations (e.g. human faces, bodies, hands, and animals). A significant part of this STAR is also devoted to classification and a high‐level comparison of the methods, as well as an overview of the datasets for training and evaluation of the discussed techniques. We conclude by discussing open challenges in the field and the social aspects associated with the usage of the reviewed methods.},
	language = {en},
	number = {2},
	urldate = {2024-10-26},
	journal = {Computer Graphics Forum},
	author = {Tretschk, Edith and Kairanda, Navami and B R, Mallikarjun and Dabral, Rishabh and Kortylewski, Adam and Egger, Bernhard and Habermann, Marc and Fua, Pascal and Theobalt, Christian and Golyanik, Vladislav},
	month = may,
	year = {2023},
	pages = {485--520},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\MFTU6ZS5\\Tretschk 等 - 2023 - State of the Art in Dense Monocular Non‐Rigid 3D R.pdf:application/pdf},
}

@article{leus_graph_2023,
	title = {Graph {Signal} {Processing}: {History}, development, impact, and outlook},
	volume = {40},
	shorttitle = {Graph {Signal} {Processing}},
	url = {https://ieeexplore.ieee.org/abstract/document/10146241/},
	number = {4},
	urldate = {2024-10-26},
	journal = {IEEE Signal Processing Magazine},
	author = {Leus, Geert and Marques, Antonio G. and Moura, José MF and Ortega, Antonio and Shuman, David I.},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {49--60},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\PWNPHQIC\\Leus 等 - 2023 - Graph Signal Processing History, development, imp.pdf:application/pdf},
}

@article{bach_challenges_2023,
	title = {Challenges and opportunities in data visualization education: {A} call to action},
	shorttitle = {Challenges and opportunities in data visualization education},
	url = {https://ieeexplore.ieee.org/abstract/document/10310184/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on visualization and computer graphics},
	author = {Bach, Benjamin and Keck, Mandy and Rajabiyazdi, Fateme and Losev, Tatiana and Meirelles, Isabel and Dykes, Jason and Laramee, Robert S. and AlKadi, Mashael and Stoiber, Christina and Huron, Samuel},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\4EVEDWE9\\Bach 等 - 2023 - Challenges and opportunities in data visualization.pdf:application/pdf},
}

@article{feng_promptmagician_2023,
	title = {Promptmagician: {Interactive} prompt engineering for text-to-image creation},
	shorttitle = {Promptmagician},
	url = {https://ieeexplore.ieee.org/abstract/document/10296017/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Yingchaojie and Wang, Xingbo and Wong, Kam Kwai and Wang, Sijia and Lu, Yuhong and Zhu, Minfeng and Wang, Baicheng and Chen, Wei},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\DCQFT774\\Feng 等 - 2023 - Promptmagician Interactive prompt engineering for.pdf:application/pdf},
}

@incollection{whitton_design_2023,
	address = {New York, NY, USA},
	edition = {1},
	title = {Design {Galleries}: {A} {General} {Approach} to {Setting} {Parameters} for {Computer} {Graphics} and {Animation}},
	isbn = {9798400708978},
	shorttitle = {Design {Galleries}},
	url = {https://dl.acm.org/doi/10.1145/3596711.3596721},
	language = {en},
	urldate = {2024-10-26},
	booktitle = {Seminal {Graphics} {Papers}: {Pushing} the {Boundaries}, {Volume} 2},
	publisher = {ACM},
	author = {Marks, J. and Andalman, B. and Beardsley, P.A. and Freeman, W. and Gibson, S. and Hodgins, J. and Kang, T. and Mirtich, B. and Pfister, H. and Ruml, W. and Ryall, K. and Seims, J. and Shieber, S.},
	editor = {Whitton, Mary C.},
	month = aug,
	year = {2023},
	doi = {10.1145/3596711.3596721},
	pages = {73--84},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\T6LXNY4R\\Marks 等 - 2023 - Design Galleries A General Approach to Setting Pa.pdf:application/pdf},
}

@article{mamurova_role_2023,
	title = {{ROLE} {AND} {APPLICATION} {OF} {COMPUTER} {GRAPHICS}},
	url = {https://openconference.us/index.php/ISPADP/article/view/537},
	urldate = {2024-10-26},
	journal = {Innovative Society: Problems, Analysis and Development Prospects (Spain)},
	author = {Mamurova, Feruza Islamovna and Khadjaeva, Nadira Sharifovna and Kadirova, Elena Vladimirovna},
	year = {2023},
	pages = {1--3},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\5XPEF4XJ\\Mamurova 等 - 2023 - ROLE AND APPLICATION OF COMPUTER GRAPHICS.pdf:application/pdf},
}

@inproceedings{mamurova_didactic_2024,
	title = {Didactic possibilities of using computer graphics programs in the educational process},
	volume = {84},
	url = {https://www.bio-conferences.org/articles/bioconf/abs/2024/03/bioconf_aquaculture2024_02020/bioconf_aquaculture2024_02020.html},
	urldate = {2024-10-26},
	booktitle = {{BIO} {Web} of {Conferences}},
	publisher = {EDP Sciences},
	author = {Mamurova, Dilfuza and Khusnidinova, Nozima},
	year = {2024},
	pages = {02020},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\J8VPIWGL\\Mamurova 和 Khusnidinova - 2024 - Didactic possibilities of using computer graphics .pdf:application/pdf},
}

@article{po_state_2024,
	title = {State of the {Art} on {Diffusion} {Models} for {Visual} {Computing}},
	volume = {43},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.15063},
	doi = {10.1111/cgf.15063},
	abstract = {Abstract
            The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion‐based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state‐of‐the‐art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion‐based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.},
	language = {en},
	number = {2},
	urldate = {2024-10-26},
	journal = {Computer Graphics Forum},
	author = {Po, R. and Yifan, W. and Golyanik, V. and Aberman, K. and Barron, J. T. and Bermano, A. and Chan, E. and Dekel, T. and Holynski, A. and Kanazawa, A. and Liu, C.K. and Liu, L. and Mildenhall, B. and Nießner, M. and Ommer, B. and Theobalt, C. and Wonka, P. and Wetzstein, G.},
	month = may,
	year = {2024},
	pages = {e15063},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\8PKMUXBM\\Po 等 - 2024 - State of the Art on Diffusion Models for Visual Co.pdf:application/pdf},
}

@article{chen_survey_2024,
	title = {A survey on graph neural networks and graph transformers in computer vision: {A} task-oriented perspective},
	shorttitle = {A survey on graph neural networks and graph transformers in computer vision},
	url = {https://ieeexplore.ieee.org/abstract/document/10638815/},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Chaoqi and Wu, Yushuang and Dai, Qiyuan and Zhou, Hong-Yu and Xu, Mutian and Yang, Sibei and Han, Xiaoguang and Yu, Yizhou},
	year = {2024},
	note = {Publisher: IEEE},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\VQ557KT4\\Chen 等 - 2024 - A survey on graph neural networks and graph transf.pdf:application/pdf},
}

@article{nyatsanga_comprehensive_2023,
	title = {A {Comprehensive} {Review} of {Data}‐{Driven} {Co}‐{Speech} {Gesture} {Generation}},
	volume = {42},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14776},
	doi = {10.1111/cgf.14776},
	abstract = {Abstract
            Gestures that accompany speech are an essential part of natural and efficient embodied human communication. The automatic generation of such co‐speech gestures is a long‐standing problem in computer animation and is considered an enabling technology for creating believable characters in film, games, and virtual social spaces, as well as for interaction with social robots. The problem is made challenging by the idiosyncratic and non‐periodic nature of human co‐speech gesture motion, and by the great diversity of communicative functions that gestures encompass. The field of gesture generation has seen surging interest in the last few years, owing to the emergence of more and larger datasets of human gesture motion, combined with strides in deep‐learning‐based generative models that benefit from the growing availability of data. This review article summarizes co‐speech gesture generation research, with a particular focus on deep generative models. First, we articulate the theory describing human gesticulation and how it complements speech. Next, we briefly discuss rule‐based and classical statistical gesture synthesis, before delving into deep learning approaches. We employ the choice of input modalities as an organizing principle, examining systems that generate gestures from audio, text and non‐linguistic input. Concurrent with the exposition of deep learning approaches, we chronicle the evolution of the related training data sets in terms of size, diversity, motion quality, and collection method (e.g., optical motion capture or pose estimation from video). Finally, we identify key research challenges in gesture generation, including data availability and quality; producing human‐like motion; grounding the gesture in the co‐occurring speech in interaction with other speakers, and in the environment; performing gesture evaluation; and integration of gesture synthesis into applications. We highlight recent approaches to tackling the various key challenges, as well as the limitations of these approaches, and point toward areas of future development.},
	language = {en},
	number = {2},
	urldate = {2024-10-26},
	journal = {Computer Graphics Forum},
	author = {Nyatsanga, S. and Kucherenko, T. and Ahuja, C. and Henter, G. E. and Neff, M.},
	month = may,
	year = {2023},
	pages = {569--596},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\K7DH5WCE\\Nyatsanga 等 - 2023 - A Comprehensive Review of Data‐Driven Co‐Speech Ge.pdf:application/pdf},
}

@article{liu_skeleton-based_2023,
	title = {Skeleton-based human action recognition via large-kernel attention graph convolutional network},
	volume = {29},
	url = {https://ieeexplore.ieee.org/abstract/document/10049725/},
	number = {5},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Yanan and Zhang, Hao and Li, Yanqiu and He, Kangjian and Xu, Dan},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {2575--2585},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\H623VQBG\\Liu 等 - 2023 - Skeleton-based human action recognition via large-.pdf:application/pdf},
}

@misc{noauthor_-_nodate,
	title = {浅谈计算机图形学的发展及应用-维普期刊 中文期刊服务平台},
	url = {http://218.28.6.71:81/Qikan/Article/Detail?id=668789083201033149&from=Qikan_Article_Detail},
	urldate = {2024-11-13},
	file = {浅谈计算机图形学的发展及应用-维普期刊 中文期刊服务平台:C\:\\Users\\Y9000X\\Zotero\\storage\\L6SBFJ3X\\Detail.html:text/html},
}

@misc{noauthor_sketch_nodate,
	title = {Sketch pad a man-machine graphical communication system {\textbar} {Proceedings} of the {SHARE} design automation workshop},
	url = {https://dl.acm.org/doi/abs/10.1145/800265.810742},
	urldate = {2024-11-13},
	file = {Sketch pad a man-machine graphical communication s.pdf:C\:\\Users\\Y9000X\\Zotero\\storage\\DM9KPADX\\Sketch pad a man-machine graphical communication s.pdf:application/pdf;Sketch pad a man-machine graphical communication system | Proceedings of the SHARE design automation workshop:C\:\\Users\\Y9000X\\Zotero\\storage\\PF43ZZ3V\\800265.html:text/html},
}

@phdthesis{_ct_2005,
	type = {{PhD} {Thesis}},
	title = {{CT} 三维重组诊断寰枢关节不全脱位的实验及临床研究},
	url = {https://core.ac.uk/download/pdf/41358204.pdf},
	urldate = {2024-11-14},
	author = {{段少银} and {蔡国祥} and {林清池} and {叶锋} and {黄锡恩} and {梁昆如}},
	year = {2005},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\ZVADIJZR\\段少银 等 - 2005 - CT 三维重组诊断寰枢关节不全脱位的实验及临床研究.pdf:application/pdf},
}

@article{__2015,
	title = {国外高校虚拟仿真实验教学现状与发展},
	volume = {34},
	url = {https://xnfz.nwu.edu.cn/file/up_document/2024/01/4mQrOc6Bt7eyLozW.pdf},
	number = {5},
	urldate = {2024-11-14},
	journal = {实验室研究与探索},
	author = {{王卫国} and {胡今鸿} and {刘宏}},
	year = {2015},
	pages = {214--219},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\G3RQY476\\王卫国 等 - 2015 - 国外高校虚拟仿真实验教学现状与发展.pdf:application/pdf},
}

@article{__2011,
	title = {浅谈计算机图形学与图形图像处理技术},
	volume = {6},
	url = {https://static.aminer.cn/upload/pdf/339/424/231/53e9cdb6b7602d97053c8061_0.pdf},
	number = {1},
	urldate = {2024-11-14},
	journal = {长春理工大学学报},
	author = {{陈敏雅} and {金旭东}},
	year = {2011},
	pages = {128--139},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\Y36G5PGJ\\陈敏雅 和 金旭东 - 2011 - 浅谈计算机图形学与图形图像处理技术.pdf:application/pdf},
}

@article{li_aads_2019,
	title = {{AADS}: {Augmented} autonomous driving simulation using data-driven algorithms},
	volume = {4},
	issn = {2470-9476},
	shorttitle = {{AADS}},
	url = {https://www.science.org/doi/10.1126/scirobotics.aaw0863},
	doi = {10.1126/scirobotics.aaw0863},
	abstract = {A scalable simulation system combines augmented images with synthesized traffic to enable training and testing of autonomous driving.
          , 
            Simulation systems have become essential to the development and validation of autonomous driving (AD) technologies. The prevailing state-of-the-art approach for simulation uses game engines or high-fidelity computer graphics (CG) models to create driving scenarios. However, creating CG models and vehicle movements (the assets for simulation) remain manual tasks that can be costly and time consuming. In addition, CG images still lack the richness and authenticity of real-world images, and using CG images for training leads to degraded performance. Here, we present our augmented autonomous driving simulation (AADS). Our formulation augmented real-world pictures with a simulated traffic flow to create photorealistic simulation images and renderings. More specifically, we used LiDAR and cameras to scan street scenes. From the acquired trajectory data, we generated plausible traffic flows for cars and pedestrians and composed them into the background. The composite images could be resynthesized with different viewpoints and sensor models (camera or LiDAR). The resulting images are photorealistic, fully annotated, and ready for training and testing of AD systems from perception to planning. We explain our system design and validate our algorithms with a number of AD tasks from detection to segmentation and predictions. Compared with traditional approaches, our method offers scalability and realism. Scalability is particularly important for AD simulations, and we believe that real-world complexity and diversity cannot be realistically captured in a virtual environment. Our augmented approach combines the flexibility of a virtual environment (e.g., vehicle movements) with the richness of the real world to allow effective simulation.},
	language = {en},
	number = {28},
	urldate = {2024-11-14},
	journal = {Science Robotics},
	author = {Li, W. and Pan, C. W. and Zhang, R. and Ren, J. P. and Ma, Y. X. and Fang, J. and Yan, F. L. and Geng, Q. C. and Huang, X. Y. and Gong, H. J. and Xu, W. W. and Wang, G. P. and Manocha, D. and Yang, R. G.},
	month = mar,
	year = {2019},
	pages = {eaaw0863},
}

@article{liu_computer_2017,
	title = {Computer architectures for autonomous driving},
	volume = {50},
	url = {https://ieeexplore.ieee.org/abstract/document/7999133/},
	number = {8},
	urldate = {2024-11-14},
	journal = {Computer},
	author = {Liu, Shaoshan and Tang, Jie and Zhang, Zhe and Gaudiot, Jean-Luc},
	year = {2017},
	note = {Publisher: IEEE},
	pages = {18--25},
}

@inproceedings{deyo_getting_1988,
	title = {Getting graphics in gear: graphics and dynamics in driving simulation},
	shorttitle = {Getting graphics in gear},
	url = {https://dl.acm.org/doi/abs/10.1145/54852.378535},
	urldate = {2024-11-14},
	booktitle = {Proceedings of the 15th annual conference on {Computer} graphics and interactive techniques},
	author = {Deyo, Rod and Briggs, John A. and Doenges, Pete},
	year = {1988},
	pages = {317--326},
}

@article{jiang_frame_2015,
	title = {Frame field generation through metric customization},
	volume = {34},
	url = {https://dl.acm.org/doi/abs/10.1145/2766927},
	number = {4},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Jiang, Tengfei and Fang, Xianzhong and Huang, Jin and Bao, Hujun and Tong, Yiying and Desbrun, Mathieu},
	year = {2015},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--11},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\Q57VLT8L\\Jiang 等 - 2015 - Frame field generation through metric customizatio.pdf:application/pdf},
}

@misc{noauthor_frame_nodate,
	title = {Frame field generation through metric customization {\textbar} {ACM} {Transactions} on {Graphics}},
	url = {https://dl.acm.org/doi/abs/10.1145/2766927},
	urldate = {2024-11-14},
}

@article{xu_sketch2scene_2013,
	title = {{Sketch2Scene}: sketch-based co-retrieval and co-placement of {3D} models},
	volume = {32},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{Sketch2Scene}},
	url = {https://dl.acm.org/doi/10.1145/2461912.2461968},
	doi = {10.1145/2461912.2461968},
	abstract = {This work presents
              Sketch2Scene
              , a framework that automatically turns a freehand sketch drawing inferring multiple scene objects to semantically valid, well arranged scenes of 3D models. Unlike the existing works on sketch-based search and composition of 3D models, which typically process individual sketched objects one by one, our technique performs
              co-retrieval
              and
              co-placement
              of 3D relevant models by jointly processing the sketched objects. This is enabled by summarizing functional and spatial relationships among models in a large collection of 3D scenes as
              structural groups
              . Our technique greatly reduces the amount of user intervention needed for sketch-based modeling of 3D scenes and fits well into the traditional production pipeline involving concept design followed by 3D modeling. A pilot study indicates that it is promising to use our technique as an alternative but more efficient tool of standard 3D modeling for 3D scene construction.},
	language = {en},
	number = {4},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics},
	author = {Xu, Kun and Chen, Kang and Fu, Hongbo and Sun, Wei-Lun and Hu, Shi-Min},
	month = jul,
	year = {2013},
	pages = {1--15},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\2CNJD2N2\\Xu 等 - 2013 - Sketch2Scene sketch-based co-retrieval and co-pla.pdf:application/pdf},
}

@article{zhang_local_2014,
	title = {Local barycentric coordinates},
	volume = {33},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2661229.2661255},
	doi = {10.1145/2661229.2661255},
	abstract = {Barycentric coordinates yield a powerful and yet simple paradigm to interpolate data values on polyhedral domains. They represent interior points of the domain as an affine combination of a set of control points, defining an interpolation scheme for any function defined on a set of control points. Numerous barycentric coordinate schemes have been proposed satisfying a large variety of properties. However, they typically define interpolation as a combination of
              all
              control points. Thus a
              local
              change in the value at a single control point will create a
              global
              change by propagation into the whole domain. In this context, we present a family of
              local barycentric coordinates
              (LBC), which select for each interior point a small set of control points and satisfy common requirements on barycentric coordinates, such as linearity, non-negativity, and smoothness. LBC are achieved through a convex optimization based on total variation, and provide a compact representation that reduces memory footprint and allows for fast deformations. Our experiments show that LBC provide more local and finer control on shape deformation than previous approaches, and lead to more intuitive deformation results.},
	language = {en},
	number = {6},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Juyong and Deng, Bailin and Liu, Zishun and Patanè, Giuseppe and Bouaziz, Sofien and Hormann, Kai and Liu, Ligang},
	month = nov,
	year = {2014},
	pages = {1--12},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\JSIDZMI2\\Zhang 等 - 2014 - Local barycentric coordinates.pdf:application/pdf},
}

@article{xiong_robust_2014,
	title = {Robust surface reconstruction via dictionary learning},
	volume = {33},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2661229.2661263},
	doi = {10.1145/2661229.2661263},
	abstract = {Surface reconstruction from point cloud is of great practical importance in computer graphics. Existing methods often realize reconstruction via a few phases with respective goals, whose integration may not give an optimal solution. In this paper, to avoid the inherent limitations of multi-phase processing in the prior art, we propose a unified framework that treats geometry and connectivity construction as one joint optimization problem. The framework is based on dictionary learning in which the dictionary consists of the vertices of the reconstructed triangular mesh and the sparse coding matrix encodes the connectivity of the mesh. The dictionary learning is formulated as a constrained
              ℓ
              2,q
              -optimization (0 {\textless}
              q
              {\textless} 1), aiming to find the vertex position and triangulation that minimize an energy function composed of point-to-mesh metric and regularization. Our formulation takes many factors into account within the same framework, including distance metric, noise/outlier resilience, sharp feature preservation, no need to estimate normal, etc., thus providing a global and robust algorithm that is able to efficiently recover a piecewise smooth surface from dense data points with imperfections. Extensive experiments using synthetic models, real world models, and publicly available benchmark show that our method outperforms the state-of-the-art in terms of accuracy, robustness to noise and outliers, geometric feature and detail preservation, and mesh connectivity.},
	language = {en},
	number = {6},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics},
	author = {Xiong, Shiyao and Zhang, Juyong and Zheng, Jianmin and Cai, Jianfei and Liu, Ligang},
	month = nov,
	year = {2014},
	pages = {1--12},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\5AF5XD2F\\Xiong 等 - 2014 - Robust surface reconstruction via dictionary learn.pdf:application/pdf},
}

@article{wang_construction_2016,
	title = {Construction of {Manifolds} via {Compatible} {Sparse} {Representations}},
	volume = {35},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2835488},
	doi = {10.1145/2835488},
	abstract = {Manifold is an important technique to model geometric objects with arbitrary topology. In this article, we propose a novel approach for constructing manifolds from discrete meshes based on sparse optimization. The local geometry for each chart is sparsely represented by a set of redundant atom functions, which have the flexibility to represent various geometries with varying smoothness. A global optimization is then proposed to guarantee compatible sparse representations in the overlapping regions of different charts. Our method can construct manifolds of varying smoothness including sharp features (creases, darts, or cusps). As an application, we can easily construct a skinning manifold surface from a given curve network. Examples show that our approach has much flexibility to generate manifold surfaces with good quality.},
	language = {en},
	number = {2},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Ruimin and Liu, Ligang and Yang, Zhouwang and Wang, Kang and Shan, Wen and Deng, Jiansong and Chen, Falai},
	month = may,
	year = {2016},
	pages = {1--10},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\C33DR7BI\\Wang 等 - 2016 - Construction of Manifolds via Compatible Sparse Re.pdf:application/pdf},
}

@article{xu_autoscanning_2015,
	title = {Autoscanning for coupled scene reconstruction and proactive object analysis},
	volume = {34},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2816795.2818075},
	doi = {10.1145/2816795.2818075},
	abstract = {Detailed scanning of indoor scenes is tedious for humans. We propose autonomous scene scanning by a robot to relieve humans from such a laborious task. In an autonomous setting, detailed scene acquisition is inevitably
              coupled
              with scene analysis at the required level of detail. We develop a framework for object-level scene reconstruction coupled with object-centric scene analysis. As a result, the autoscanning and reconstruction will be
              object-aware
              , guided by the object analysis. The analysis is, in turn, gradually improved with progressively increased object-wise data fidelity. In realizing such a framework, we drive the robot to execute an iterative
              analyze-and-validate
              algorithm which interleaves between object analysis and guided validations.
            
            
              The object analysis incorporates online learning into a robust graph-cut based segmentation framework, achieving a global update of object-level segmentation based on the knowledge gained from robot-operated local validation. Based on the current analysis, the robot performs
              proactive
              validation over the scene with physical push and scan refinement, aiming at reducing the uncertainty of both object-level segmentation and object-wise reconstruction. We propose a joint entropy to measure such uncertainty based on segmentation confidence and reconstruction quality, and formulate the selection of validation actions as a maximum information gain problem. The output of our system is a reconstructed scene with both object extraction and object-wise geometry fidelity.},
	language = {en},
	number = {6},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics},
	author = {Xu, Kai and Huang, Hui and Shi, Yifei and Li, Hao and Long, Pinxin and Caichen, Jianong and Sun, Wei and Chen, Baoquan},
	month = nov,
	year = {2015},
	pages = {1--14},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\UFS3QBU6\\Xu 等 - 2015 - Autoscanning for coupled scene reconstruction and .pdf:application/pdf},
}

@article{li_cubic_2013,
	title = {Cubic mean value coordinates.},
	volume = {32},
	url = {http://cg.cs.tsinghua.edu.cn/people/~xianying/Papers/CubicMVCs/cubic(2.9M).pdf},
	number = {4},
	urldate = {2024-11-14},
	journal = {ACM Trans. Graph.},
	author = {Li, Xian-Ying and Ju, Tao and Hu, Shi-Min},
	year = {2013},
	pages = {126--1},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\T2VNIZBV\\Li 等 - 2013 - Cubic mean value coordinates..pdf:application/pdf},
}

@article{li_space-time_2014,
	title = {Space-time editing of elastic motion through material optimization and reduction},
	volume = {33},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2601097.2601217},
	doi = {10.1145/2601097.2601217},
	abstract = {We present a novel method for elastic animation editing with space-time constraints. In a sharp departure from previous approaches, we not only optimize control forces added to a linearized dynamic model, but also optimize material properties to better match user constraints and provide plausible and consistent motion. Our approach achieves efficiency and scalability by performing all computations in a reduced rotation-strain (RS) space constructed with both cubature and geometric reduction, leading to two orders of magnitude improvement over the original RS method. We demonstrate the utility and versatility of our method in various applications, including motion editing, pose interpolation, and estimation of material parameters from existing animation sequences.},
	language = {en},
	number = {4},
	urldate = {2024-11-14},
	journal = {ACM Transactions on Graphics},
	author = {Li, Siwang and Huang, Jin and De Goes, Fernando and Jin, Xiaogang and Bao, Hujun and Desbrun, Mathieu},
	month = jul,
	year = {2014},
	pages = {1--10},
	file = {Available Version (via Google Scholar):C\:\\Users\\Y9000X\\Zotero\\storage\\N6MHX8JY\\Li 等 - 2014 - Space-time editing of elastic motion through mater.pdf:application/pdf},
}
